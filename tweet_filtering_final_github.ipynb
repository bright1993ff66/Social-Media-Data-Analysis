{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import pytz\n",
    "import tweepy\n",
    "import time\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from datetime import datetime\n",
    "# A package which could be used to check whether an account is a bot\n",
    "import botometer\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_2018_path_raw = r\"XXXXX\"\n",
    "tweet_2017_path_raw = r\"XXXXX\"\n",
    "tweet_2016_path_raw = r\"XXXXX\"\n",
    "tweet_2017_path = r\"XXXXX\"\n",
    "tweet_combined_path = r\"XXXXX\"\n",
    "check_file_path = r\"XXXXX\"\n",
    "desktop = r\"XXXXX\"\n",
    "saving_path = r\"XXXXX\"\n",
    "tweet_filtering_path = r\"XXXXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hong Kong and Shanghai share the same time zone.\n",
    "# Hence, we transform the utc time in our dataset into Shanghai time\n",
    "time_zone_hk = pytz.timezone('Asia/Shanghai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some account information\n",
    "# For more details, please go to: https://github.com/IUNetSci/botometer-python\n",
    "mashape_key = \"XXXXX\"\n",
    "twitter_app_auth = {\n",
    "    'consumer_key': \"XXXXX\",\n",
    "    'consumer_secret': \"XXXXX\",\n",
    "    'access_token': \"XXXXX\",\n",
    "    'access_token_secret': \"XXXXX\",\n",
    "  }\n",
    "bom = botometer.Botometer(wait_on_ratelimit=True,\n",
    "                          mashape_key=mashape_key,\n",
    "                          **twitter_app_auth)\n",
    "\n",
    "auth = OAuthHandler(\"XXXXX\", \"XXXXX\")\n",
    "\n",
    "auth.set_access_token(\"XXXXX\", \"XXXXX\")\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Construct the API instance\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Firstly, define some functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read multiple csv files from a local directory\n",
    "def read_text_from_multi_csvs(path):\n",
    "    all_csv_files = os.listdir(path)\n",
    "    dataframes = []\n",
    "    for file in all_csv_files:\n",
    "        dataframe = pd.read_csv(os.path.join(path, file), encoding='latin-1', dtype='str', \n",
    "                                quoting=csv.QUOTE_NONNUMERIC)\n",
    "        dataframes.append(dataframe)\n",
    "    combined_dataframes = pd.concat(dataframes, sort=True)\n",
    "    return combined_dataframes\n",
    "\n",
    "# Function used to output a pandas dataframe for each user based on the user account number\n",
    "def derive_dataframe_for_each_user(df, all_users):\n",
    "    dataframes = []\n",
    "    for user in all_users:\n",
    "        dataframes.append(df.loc[df['user_id_str']==user])\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "# Based on the dataframe for each user, compute the time range between his or her first tweet and last tweet\n",
    "def compute_time_range_for_one_user(df):\n",
    "    user_id_str = list(df['user_id_str'])[0]\n",
    "    first_row_time_object = list(df.head(1)['hk_time'])[0]\n",
    "    end_row_time_object = list(df.tail(1)['hk_time'])[0]\n",
    "    time_range = end_row_time_object - first_row_time_object\n",
    "    return (user_id_str, time_range.days)\n",
    "\n",
    "# Add a new colume named hk_time\n",
    "def get_hk_time(df):\n",
    "    changed_time_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        time_to_change = datetime.strptime(row['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "        # get the hk time\n",
    "        changed_time = time_to_change.astimezone(time_zone_hk)\n",
    "        changed_time_list.append(changed_time)\n",
    "    df['hk_time'] = changed_time_list\n",
    "    return df\n",
    "\n",
    "def check_id_diff(tweet_id_set, bot_id_set):\n",
    "    wrong_id_list = []\n",
    "    for index in tweet_id_set:\n",
    "        if index in bot_id_set:\n",
    "            pass\n",
    "        else:\n",
    "            wrong_id_list.append(index)\n",
    "    return wrong_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_bots_have_same_geoinformation(df, prop_threshold=0.70):\n",
    "    users = set(list(df['user_id_str']))\n",
    "    bot_account = []\n",
    "    for user in users:\n",
    "        dataframe = df.loc[df['user_id_str']==user]\n",
    "        lat_counter = Counter(dataframe['lat'])\n",
    "        lon_counter = Counter(dataframe['lon'])\n",
    "        decide = (compute_the_highest_proportion_from_counter(lat_counter, prop_threshold)) or (compute_the_highest_proportion_from_counter(lon_counter, prop_threshold))\n",
    "        # If only one unqiue geoinformation is found and more than 10 tweets are posted, we regard this account as bot\n",
    "        if decide:\n",
    "            bot_account.append(user)\n",
    "        else:\n",
    "            pass\n",
    "    cleaned_df = df.loc[~df['user_id_str'].isin(bot_account)]\n",
    "#     cleaned_df.to_pickle(os.path.join(saving_path, file_name))\n",
    "    return cleaned_df\n",
    "\n",
    "def compute_the_highest_proportion_from_counter(counter_dict, prop_threshold):\n",
    "    total_count = sum(counter_dict.values())\n",
    "    result = False\n",
    "    for latitude in list(counter_dict.keys()):\n",
    "        if counter_dict[latitude]/total_count > prop_threshold:\n",
    "            result = True\n",
    "            return result\n",
    "        else:\n",
    "            pass\n",
    "    return result\n",
    "\n",
    "def number_of_tweet_user(df):\n",
    "    user_num = len(set(df['user_id_str']))\n",
    "    tweet_num = df.shape[0]\n",
    "    print('Total number of tweet is: {}; Total number of user is {}'.format(tweet_num, user_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bot(id_str):\n",
    "    result = bom.check_account(id_str)\n",
    "    return result['cap']['universal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read raw tweets files\n",
    "\n",
    "### 2.1 Load the tweet 2018 raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_dataframe_2018 = read_text_from_multi_csvs(path=tweet_2018_path_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweet is: 1323013; Total number of user is 95832\n"
     ]
    }
   ],
   "source": [
    "number_of_tweet_user(combined_dataframe_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load the tweet 2017 raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_dataframe_2017 = read_text_from_multi_csvs(path=tweet_2017_path_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweet is: 1602627; Total number of user is 115846\n"
     ]
    }
   ],
   "source": [
    "number_of_tweet_user(combined_dataframe_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load the tweet 2016 raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_dataframe_2016 = read_text_from_multi_csvs(path=tweet_2016_path_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweet is: 1281281; Total number of user is 84029\n"
     ]
    }
   ],
   "source": [
    "number_of_tweet_user(combined_dataframe_2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Combine the 2016 and 2017 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_name_list = ['created_at', 'id_str', 'lang', 'lat', \n",
    "       'lon', 'place_id', 'place_lat', 'place_lon', 'place_name', 'text', 'time_zone', 'truncated', 'url',\n",
    "       'user_created_at', 'user_id_str', 'user_lang', 'user_url',\n",
    "       'verified']\n",
    "len(selected_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataframe_2018 = combined_dataframe_2018[selected_name_list]\n",
    "combined_dataframe_2017 = combined_dataframe_2017[selected_name_list]\n",
    "combined_dataframe_2016 = combined_dataframe_2016[selected_name_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe = pd.concat([combined_dataframe_2018, combined_dataframe_2017, combined_dataframe_2016], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweet is: 4206921; Total number of user is 249912\n"
     ]
    }
   ],
   "source": [
    "number_of_tweet_user(total_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe.to_csv(os.path.join(tweet_combined_path, 'tweet_combined.csv'), encoding='utf-8', \n",
    "                       quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del combined_dataframe_2018\n",
    "del combined_dataframe_2017\n",
    "del combined_dataframe_2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Some initial steps\n",
    "\n",
    "- Only consider the English and Chinese tweets\n",
    "- Delete the verified accounts\n",
    "- Remove tweets which don't have latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2467054, 18)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataframe_zh_en = total_dataframe.loc[total_dataframe['lang'].isin(['zh', 'en'])]\n",
    "total_dataframe_zh_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'FALSE': 2392527, 'TRUE': 74527})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(total_dataframe_zh_en['verified'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2392527, 18)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataframe_without_verified = total_dataframe_zh_en.loc[total_dataframe_zh_en['verified'].isin(['FALSE'])]\n",
    "total_dataframe_without_verified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762646, 18)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataframe_with_geo = total_dataframe_without_verified.dropna(axis=0, subset=['lat'])\n",
    "total_dataframe_with_geo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could save the file to a local directory by:\n",
    "\n",
    "```Python\n",
    "total_dataframe_with_geo.to_csv(os.path.join(desktop, 'dataframe.csv'), encoding='utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_with_geo.to_csv(os.path.join(tweet_combined_path, 'total_dataframe_with_geo.csv'), encoding='utf-8', \n",
    "                                quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create timestamps for each tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the hk time of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_with_geo_copy = total_dataframe_with_geo.copy()\n",
    "total_dataframe_with_hk_time = get_hk_time(total_dataframe_with_geo_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_with_hk_time['year'] = total_dataframe_with_hk_time.apply(lambda row: int(row['hk_time'].year), axis=1)\n",
    "total_dataframe_with_hk_time['month'] = total_dataframe_with_hk_time.apply(lambda row: int(row['hk_time'].month), axis=1)\n",
    "total_dataframe_with_hk_time['month_plus_year'] = total_dataframe_with_hk_time.apply(\n",
    "    lambda row: str(row['year'])+'_'+str(row['month']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_with_hk_time['day'] = total_dataframe_with_hk_time.apply(lambda row: int(row['hk_time'].day), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hk_time</th>\n",
       "      <th>created_at</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>month_plus_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 07:00:08+08:00</td>\n",
       "      <td>Sun Dec 31 23:00:08 +0000 2017</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-01 07:02:03+08:00</td>\n",
       "      <td>Sun Dec 31 23:02:03 +0000 2017</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-01 07:03:06+08:00</td>\n",
       "      <td>Sun Dec 31 23:03:06 +0000 2017</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01-01 07:04:05+08:00</td>\n",
       "      <td>Sun Dec 31 23:04:05 +0000 2017</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-01-01 07:05:03+08:00</td>\n",
       "      <td>Sun Dec 31 23:05:03 +0000 2017</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-01-01 07:05:57+08:00</td>\n",
       "      <td>Sun Dec 31 23:05:57 +0000 2017</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-01-01 07:10:17+08:00</td>\n",
       "      <td>Sun Dec 31 23:10:17 +0000 2017</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-01-01 07:13:25+08:00</td>\n",
       "      <td>Sun Dec 31 23:13:25 +0000 2017</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018-01-01 07:14:25+08:00</td>\n",
       "      <td>Sun Dec 31 23:14:25 +0000 2017</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2018-01-01 07:15:51+08:00</td>\n",
       "      <td>Sun Dec 31 23:15:51 +0000 2017</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     hk_time                      created_at  year  month  \\\n",
       "1  2018-01-01 07:00:08+08:00  Sun Dec 31 23:00:08 +0000 2017  2018      1   \n",
       "5  2018-01-01 07:02:03+08:00  Sun Dec 31 23:02:03 +0000 2017  2018      1   \n",
       "9  2018-01-01 07:03:06+08:00  Sun Dec 31 23:03:06 +0000 2017  2018      1   \n",
       "10 2018-01-01 07:04:05+08:00  Sun Dec 31 23:04:05 +0000 2017  2018      1   \n",
       "11 2018-01-01 07:05:03+08:00  Sun Dec 31 23:05:03 +0000 2017  2018      1   \n",
       "13 2018-01-01 07:05:57+08:00  Sun Dec 31 23:05:57 +0000 2017  2018      1   \n",
       "25 2018-01-01 07:10:17+08:00  Sun Dec 31 23:10:17 +0000 2017  2018      1   \n",
       "29 2018-01-01 07:13:25+08:00  Sun Dec 31 23:13:25 +0000 2017  2018      1   \n",
       "32 2018-01-01 07:14:25+08:00  Sun Dec 31 23:14:25 +0000 2017  2018      1   \n",
       "35 2018-01-01 07:15:51+08:00  Sun Dec 31 23:15:51 +0000 2017  2018      1   \n",
       "\n",
       "    day month_plus_year  \n",
       "1     1          2018_1  \n",
       "5     1          2018_1  \n",
       "9     1          2018_1  \n",
       "10    1          2018_1  \n",
       "11    1          2018_1  \n",
       "13    1          2018_1  \n",
       "25    1          2018_1  \n",
       "29    1          2018_1  \n",
       "32    1          2018_1  \n",
       "35    1          2018_1  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataframe_with_hk_time.head(10)[['hk_time', 'created_at', 'year', 'month', 'day', 'month_plus_year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then**, sort the dataset based on the hk time column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_with_hk_time_sorted = total_dataframe_with_hk_time.sort_values(by='hk_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hk_time</th>\n",
       "      <th>created_at</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>month_plus_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-12-19 05:21:18+08:00</td>\n",
       "      <td>Tue Dec 18 21:21:18 +0000 2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>2018_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-12-19 05:47:11+08:00</td>\n",
       "      <td>Tue Dec 18 21:47:11 +0000 2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>2018_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-12-19 05:54:21+08:00</td>\n",
       "      <td>Tue Dec 18 21:54:21 +0000 2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>2018_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-19 06:00:35+08:00</td>\n",
       "      <td>Tue Dec 18 22:00:35 +0000 2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>2018_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-12-19 06:32:24+08:00</td>\n",
       "      <td>Tue Dec 18 22:32:24 +0000 2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>2018_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-12-19 06:43:05+08:00</td>\n",
       "      <td>Tue Dec 18 22:43:05 +0000 2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>2018_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-12-19 06:49:55+08:00</td>\n",
       "      <td>Tue Dec 18 22:49:55 +0000 2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>2018_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-12-19 06:51:54+08:00</td>\n",
       "      <td>Tue Dec 18 22:51:54 +0000 2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>2018_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-12-19 06:54:09+08:00</td>\n",
       "      <td>Tue Dec 18 22:54:09 +0000 2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>2018_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-12-19 06:57:02+08:00</td>\n",
       "      <td>Tue Dec 18 22:57:02 +0000 2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>2018_12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     hk_time                      created_at  year  month  \\\n",
       "7  2018-12-19 05:21:18+08:00  Tue Dec 18 21:21:18 +0000 2018  2018     12   \n",
       "13 2018-12-19 05:47:11+08:00  Tue Dec 18 21:47:11 +0000 2018  2018     12   \n",
       "15 2018-12-19 05:54:21+08:00  Tue Dec 18 21:54:21 +0000 2018  2018     12   \n",
       "0  2018-12-19 06:00:35+08:00  Tue Dec 18 22:00:35 +0000 2018  2018     12   \n",
       "13 2018-12-19 06:32:24+08:00  Tue Dec 18 22:32:24 +0000 2018  2018     12   \n",
       "20 2018-12-19 06:43:05+08:00  Tue Dec 18 22:43:05 +0000 2018  2018     12   \n",
       "24 2018-12-19 06:49:55+08:00  Tue Dec 18 22:49:55 +0000 2018  2018     12   \n",
       "25 2018-12-19 06:51:54+08:00  Tue Dec 18 22:51:54 +0000 2018  2018     12   \n",
       "27 2018-12-19 06:54:09+08:00  Tue Dec 18 22:54:09 +0000 2018  2018     12   \n",
       "31 2018-12-19 06:57:02+08:00  Tue Dec 18 22:57:02 +0000 2018  2018     12   \n",
       "\n",
       "    day month_plus_year  \n",
       "7    19         2018_12  \n",
       "13   19         2018_12  \n",
       "15   19         2018_12  \n",
       "0    19         2018_12  \n",
       "13   19         2018_12  \n",
       "20   19         2018_12  \n",
       "24   19         2018_12  \n",
       "25   19         2018_12  \n",
       "27   19         2018_12  \n",
       "31   19         2018_12  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataframe_with_hk_time_sorted.tail(10)[['hk_time', 'created_at', 'year', 'month', 'day', 'month_plus_year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_with_hk_time_sorted.to_csv(os.path.join(tweet_combined_path, 'tweet_geocoded.csv'), encoding='utf-8', \n",
    "                                          quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_with_hk_time_sorted = pd.read_csv(os.path.join(tweet_combined_path, 'tweet_geocoded.csv'), encoding='utf-8', \n",
    "                                          quoting=csv.QUOTE_NONNUMERIC, dtype='str', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762646, 23)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataframe_with_hk_time_sorted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the latitude & longitude of total dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_with_hk_time_sorted_geo = total_dataframe_with_hk_time_sorted[['lat', 'lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_with_hk_time_sorted_geo_copy = total_dataframe_with_hk_time_sorted_geo.copy()\n",
    "total_dataframe_with_hk_time_sorted_geo_copy['index_num'] = list(range(0, total_dataframe_with_hk_time_sorted_geo_copy.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>index_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22.2788499</td>\n",
       "      <td>114.18462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>22.31530176</td>\n",
       "      <td>113.9348316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22.27680815</td>\n",
       "      <td>113.9161873</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.27564274</td>\n",
       "      <td>114.1711743</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22.270978</td>\n",
       "      <td>113.576678</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            lat          lon  index_num\n",
       "12   22.2788499    114.18462          0\n",
       "19  22.31530176  113.9348316          1\n",
       "20  22.27680815  113.9161873          2\n",
       "21  22.27564274  114.1711743          3\n",
       "23    22.270978   113.576678          4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataframe_with_hk_time_sorted_geo_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_with_hk_time_sorted_geo_copy.to_csv(os.path.join(tweet_combined_path, 'tweet_combined_geoinfo.csv'), \n",
    "                                                   encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762646, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataframe_with_hk_time_sorted_geo_copy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "release some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "del total_dataframe_with_geo_copy\n",
    "del total_dataframe_with_hk_time\n",
    "del total_dataframe_with_hk_time_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use ArcMap to find all the tweets posted in Hong Kong\n",
    "\n",
    "Load the spatial join result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudinal_path = os.path.join(tweet_combined_path, 'shapefiles', 'longitudinal')\n",
    "cross_sectional_path = os.path.join(tweet_combined_path, 'shapefiles', 'cross_sectional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudinal_result_dataframe = pd.DataFrame(columns=['lat', 'lon', 'TPU', 'index_num'])\n",
    "with open(os.path.join(longitudinal_path, 'longitudinal_spatial_join_result.txt'), encoding='utf-8') as longitudinal_data:\n",
    "    header = longitudinal_data.readline()\n",
    "    tweet_lines = longitudinal_data.readlines()\n",
    "    lat_list = []\n",
    "    lon_list = []\n",
    "    longitudinal_tpu_list = []\n",
    "    index_num_list = []\n",
    "    for line in tweet_lines:\n",
    "        line_list = line.split(',')\n",
    "        lat_list.append(line_list[3])\n",
    "        lon_list.append(line_list[4])\n",
    "        longitudinal_tpu_list.append(line_list[7])\n",
    "        index_num_list.append(line_list[5])\n",
    "    longitudinal_result_dataframe['lat'] = lat_list\n",
    "    longitudinal_result_dataframe['lon'] = lon_list\n",
    "    longitudinal_result_dataframe['TPU'] = longitudinal_tpu_list\n",
    "    longitudinal_result_dataframe['index_num'] = index_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_sectional_result_dataframe = pd.DataFrame(columns=['lat', 'lon', 'TPU', 'index_num'])\n",
    "with open(os.path.join(cross_sectional_path, 'cross_sectional_tpu_info.txt'), 'r', encoding='utf-8') as cross_sectional_data:\n",
    "    header = cross_sectional_data.readline()\n",
    "    tweet_lines = cross_sectional_data.readlines()\n",
    "    lat_list = []\n",
    "    lon_list = []\n",
    "    cross_sectional_tpu_list = []\n",
    "    index_num_list = []\n",
    "    for line in tweet_lines:\n",
    "        line_list = line.split(',')\n",
    "        lat_list.append(line_list[3])\n",
    "        lon_list.append(line_list[4])\n",
    "        cross_sectional_tpu_list.append(line_list[7][:-1])\n",
    "        index_num_list.append(line_list[5])\n",
    "    cross_sectional_result_dataframe['lat'] = lat_list\n",
    "    cross_sectional_result_dataframe['lon'] = lon_list\n",
    "    cross_sectional_result_dataframe['TPU'] = cross_sectional_tpu_list\n",
    "    cross_sectional_result_dataframe['index_num'] = index_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpu_info_longitudinal_path = os.path.join(tweet_combined_path, 'tpu_info_dataframes', 'longitudinal')\n",
    "tpu_info_cross_sectional_path = os.path.join(tweet_combined_path, 'tpu_info_dataframes', 'cross_sectional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_sectional_result_dataframe['TPU'] = cross_sectional_result_dataframe['TPU'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudinal_result_dataframe.to_csv(os.path.join(tpu_info_longitudinal_path, 'tpu_info_longitudinal.csv'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpu_name_match_dict = {'121': '121 and 123 - 124',\n",
    " '146': '146 - 147',\n",
    " '156': '156 and 158',\n",
    " '164': '164 - 165',\n",
    " '175': '175 - 176',\n",
    " '181': '181 - 182',\n",
    " '183': '183 - 184',\n",
    " '193': '193, 195 and 198',\n",
    " '194': '190, 192 and 194',\n",
    " '216': '213 and 215 - 216',\n",
    " '251': '251 and 256',\n",
    " '255': '255 and 269',\n",
    " '288': '288 - 289',\n",
    " '293': '293 and 296',\n",
    " '310': '310 and 321',\n",
    " '320': '320, 324 and 329',\n",
    " '340': '331 - 334, 336 and 340',\n",
    " '411': '411 - 416 and 427',\n",
    " '421': '421 - 422',\n",
    " '423': '423 and 428',\n",
    " '431': '431 - 434',\n",
    " '543': '543 and 546',\n",
    " '610': '610, 621 and 632',\n",
    " '620': '620, 622 and 641',\n",
    " '631': '631 and 633',\n",
    " '651': '651 - 653',\n",
    " '711': '711 - 712, 721 and 728',\n",
    " '722': '722 and 727',\n",
    " '731': '731, 733 and 754',\n",
    " '732': '732, 751 and 753',\n",
    " '741': '741 - 744',\n",
    " '756': '756 and 761 - 762',\n",
    " '811': '811 - 815',\n",
    " '824': '824 and 829',\n",
    " '826': '826 and 828',\n",
    " '832': '832 and 834',\n",
    " '911': '911 - 913',\n",
    " '931': '931 and 933',\n",
    " '932': '932 and 934',\n",
    " '941': '941 - 943',\n",
    " '950': '950 - 951',\n",
    " '961': '961 - 963',\n",
    " '971': '971 - 974'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_sectional_result_dataframe_social_demographic_tpu = cross_sectional_result_dataframe.replace({'TPU':tpu_name_match_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_sectional_result_dataframe_social_demographic_tpu.to_csv(os.path.join(tpu_info_cross_sectional_path, \n",
    "                                                                            'tpu_cross_sectional_info.csv'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_sectional_result_dataframe_social_demographic_tpu = pd.read_csv(os.path.join(tpu_info_cross_sectional_path, \n",
    "                                                                            'tpu_cross_sectional_info.csv'), encoding='utf-8')\n",
    "longitudinal_result_dataframe = pd.read_csv(os.path.join(tpu_info_longitudinal_path, 'tpu_info_longitudinal.csv'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_sectional_result_dataframe_social_demographic_tpu_sorted = cross_sectional_result_dataframe_social_demographic_tpu.sort_values(by='index_num')\n",
    "longitudinal_result_dataframe_tpu_sorted = longitudinal_result_dataframe.sort_values(by='index_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762646, 5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longitudinal_result_dataframe_tpu_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762646, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_sectional_result_dataframe_social_demographic_tpu_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_with_hk_time_sorted['TPU_longitudinal'] = list(longitudinal_result_dataframe_tpu_sorted['TPU'])\n",
    "total_dataframe_with_hk_time_sorted['TPU_cross_sectional'] = list(cross_sectional_result_dataframe_social_demographic_tpu_sorted['TPU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id_str</th>\n",
       "      <th>lang</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>place_id</th>\n",
       "      <th>place_lat</th>\n",
       "      <th>place_lon</th>\n",
       "      <th>place_name</th>\n",
       "      <th>text</th>\n",
       "      <th>...</th>\n",
       "      <th>user_lang</th>\n",
       "      <th>user_url</th>\n",
       "      <th>verified</th>\n",
       "      <th>hk_time</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_plus_year</th>\n",
       "      <th>day</th>\n",
       "      <th>TPU_longitudinal</th>\n",
       "      <th>TPU_cross_sectional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sat May 07 06:18:59 +0000 2016</td>\n",
       "      <td>7.28831E+17</td>\n",
       "      <td>en</td>\n",
       "      <td>22.2788499</td>\n",
       "      <td>114.18462</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.271674</td>\n",
       "      <td>114.185178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Working #Saturday #Afternoon! #Final #Touch i...</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>http://www.facebook.com/derekhysteric525</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2016-05-07 14:18:59+08:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>5</td>\n",
       "      <td>2016_5</td>\n",
       "      <td>7</td>\n",
       "      <td>146</td>\n",
       "      <td>146 - 147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sat May 07 07:02:19 +0000 2016</td>\n",
       "      <td>7.28842E+17</td>\n",
       "      <td>en</td>\n",
       "      <td>22.31530176</td>\n",
       "      <td>113.9348316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.2465325</td>\n",
       "      <td>114.064237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm at Hong Kong International Airport &lt;U+9999...</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>http://manishmaurya89.blogspot.com/?m=1</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2016-05-07 15:02:19+08:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>5</td>\n",
       "      <td>2016_5</td>\n",
       "      <td>7</td>\n",
       "      <td>951</td>\n",
       "      <td>950 - 951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sat May 07 07:02:34 +0000 2016</td>\n",
       "      <td>7.28842E+17</td>\n",
       "      <td>en</td>\n",
       "      <td>22.27680815</td>\n",
       "      <td>113.9161873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.2465325</td>\n",
       "      <td>114.064237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The cable car ride... #cablecar #mountain #360...</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>http://kotakitam.wordpress.com</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2016-05-07 15:02:34+08:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>5</td>\n",
       "      <td>2016_5</td>\n",
       "      <td>7</td>\n",
       "      <td>943</td>\n",
       "      <td>941 - 943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sat May 07 07:03:11 +0000 2016</td>\n",
       "      <td>7.28843E+17</td>\n",
       "      <td>en</td>\n",
       "      <td>22.27564274</td>\n",
       "      <td>114.1711743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.271674</td>\n",
       "      <td>114.185178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love Roses! &lt;ed&gt;&lt;U+00A0&gt;&lt;U+00BD&gt;&lt;ed&gt;&lt;U+00B8&gt;&lt;U...</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>http://www.denkipenki.vsco.co</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2016-05-07 15:03:11+08:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>5</td>\n",
       "      <td>2016_5</td>\n",
       "      <td>7</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sat May 07 07:03:28 +0000 2016</td>\n",
       "      <td>7.28843E+17</td>\n",
       "      <td>en</td>\n",
       "      <td>22.270978</td>\n",
       "      <td>113.576678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.869936</td>\n",
       "      <td>113.4197245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're #hiring! Read about our latest #job open...</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>http://www.careerarc.com/job-seeker</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2016-05-07 15:03:28+08:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>5</td>\n",
       "      <td>2016_5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_at       id_str lang          lat  \\\n",
       "12  Sat May 07 06:18:59 +0000 2016  7.28831E+17   en   22.2788499   \n",
       "19  Sat May 07 07:02:19 +0000 2016  7.28842E+17   en  22.31530176   \n",
       "20  Sat May 07 07:02:34 +0000 2016  7.28842E+17   en  22.27680815   \n",
       "21  Sat May 07 07:03:11 +0000 2016  7.28843E+17   en  22.27564274   \n",
       "23  Sat May 07 07:03:28 +0000 2016  7.28843E+17   en    22.270978   \n",
       "\n",
       "            lon place_id   place_lat    place_lon place_name  \\\n",
       "12    114.18462      NaN   22.271674   114.185178        NaN   \n",
       "19  113.9348316      NaN  22.2465325   114.064237        NaN   \n",
       "20  113.9161873      NaN  22.2465325   114.064237        NaN   \n",
       "21  114.1711743      NaN   22.271674   114.185178        NaN   \n",
       "23   113.576678      NaN   22.869936  113.4197245        NaN   \n",
       "\n",
       "                                                 text         ...          \\\n",
       "12  #Working #Saturday #Afternoon! #Final #Touch i...         ...           \n",
       "19  I'm at Hong Kong International Airport <U+9999...         ...           \n",
       "20  The cable car ride... #cablecar #mountain #360...         ...           \n",
       "21  Love Roses! <ed><U+00A0><U+00BD><ed><U+00B8><U...         ...           \n",
       "23  We're #hiring! Read about our latest #job open...         ...           \n",
       "\n",
       "   user_lang                                  user_url verified  \\\n",
       "12        en  http://www.facebook.com/derekhysteric525    FALSE   \n",
       "19        en   http://manishmaurya89.blogspot.com/?m=1    FALSE   \n",
       "20        en            http://kotakitam.wordpress.com    FALSE   \n",
       "21        en             http://www.denkipenki.vsco.co    FALSE   \n",
       "23        en       http://www.careerarc.com/job-seeker    FALSE   \n",
       "\n",
       "                      hk_time  year month month_plus_year day  \\\n",
       "12  2016-05-07 14:18:59+08:00  2016     5          2016_5   7   \n",
       "19  2016-05-07 15:02:19+08:00  2016     5          2016_5   7   \n",
       "20  2016-05-07 15:02:34+08:00  2016     5          2016_5   7   \n",
       "21  2016-05-07 15:03:11+08:00  2016     5          2016_5   7   \n",
       "23  2016-05-07 15:03:28+08:00  2016     5          2016_5   7   \n",
       "\n",
       "   TPU_longitudinal TPU_cross_sectional  \n",
       "12              146           146 - 147  \n",
       "19              951           950 - 951  \n",
       "20              943           941 - 943  \n",
       "21              131                 131  \n",
       "23                0                   0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataframe_with_hk_time_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_with_hk_time_sorted.to_csv(os.path.join(tweet_combined_path, 'total_dataframe_with_tpuinfo.csv'), \n",
    "                                          encoding='utf-8', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_in_hk = total_dataframe_with_hk_time_sorted.loc[total_dataframe_with_hk_time_sorted['TPU_longitudinal'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_in_hk.to_csv(os.path.join(tweet_combined_path, 'total_dataframe_in_hk.csv'), \n",
    "                                          encoding='utf-8', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweet is: 562578; Total number of user is 62820\n"
     ]
    }
   ],
   "source": [
    "number_of_tweet_user(total_dataframe_in_hk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Delete bots\n",
    "\n",
    "### 6.1 Pass through my own function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 12min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "total_dataframe_without_bot_step1 = delete_bots_have_same_geoinformation(total_dataframe_in_hk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweet is: 445972; Total number of user is 31679\n"
     ]
    }
   ],
   "source": [
    "number_of_tweet_user(total_dataframe_without_bot_step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'en': 332848, 'zh': 113124})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(total_dataframe_without_bot_step1['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweet is: 445972; Total number of user is 31679\n"
     ]
    }
   ],
   "source": [
    "number_of_tweet_user(total_dataframe_without_bot_step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_without_bot_step1.to_csv(os.path.join(tweet_combined_path, 'tweet_without_bot_step1.csv'), encoding='utf-8', \n",
    "                                        quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_without_bot_step1 = pd.read_csv(os.path.join(tweet_combined_path, 'tweet_without_bot_step1.csv'), encoding='utf-8', \n",
    "                                        quoting=csv.QUOTE_NONNUMERIC, dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445972, 26)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataframe_without_bot_step1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Use botometer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_list = list(set(total_dataframe_without_bot_step1['user_id_str']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check bots starts....\n"
     ]
    }
   ],
   "source": [
    "print('Check bots starts....')\n",
    "bot_result_list = []\n",
    "processed_account_list = []\n",
    "account_with_error = []\n",
    "# The input of the check bot function should be integers\n",
    "account_integers = [int(number) for number in user_id_list]\n",
    "# Get a set of unique users and transform it to list\n",
    "account_integer_set_list = list(set(account_integers))\n",
    "account_integer_set_string_list = [str(num) for num in account_integer_set_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run the following codes to use bototmeter API to do the tweet filtering:\n",
    "\n",
    "```Python\n",
    "botometer_check_path = r'F:\\CityU\\Datasets\\Hong Kong Tweets Combined\\botometer_check'\n",
    "\n",
    "assert len(bot_result_list) == 0\n",
    "assert len(processed_account_list) == 0\n",
    "assert len(account_with_error) == 0\n",
    "\n",
    "for index, user in enumerate(account_integer_set_list):\n",
    "    print('-----------------------------------------------')\n",
    "    print(\"Coping with the \", index+1, 'th user: ', user)\n",
    "    try:\n",
    "        bot_likelihood = check_bot(int(user))\n",
    "        print('The botlikelihood score is: {}'.format(bot_likelihood))\n",
    "        bot_result_list.append(bot_likelihood)\n",
    "        processed_account_list.append(user)\n",
    "        print('This account could be processed...')\n",
    "        print('-----------------------------------------------')\n",
    "    except Exception as e:\n",
    "        # In this case, the api shows that this page is not authorized or does not exit\n",
    "        # We record these accouts as Not Authorized in the bot_result_list\n",
    "        bot_result_list.append('Not Authorized')\n",
    "        processed_account_list.append(user)\n",
    "        account_with_error.append(user)\n",
    "        print('This account has some problem...')\n",
    "        print('-----------------------------------------------')\n",
    "    assert len(processed_account_list) == len(bot_result_list)\n",
    "    if ((index+1) % 1000 == 0):\n",
    "        print('The first {} users have been processed'.format(index+1))\n",
    "        print('Have a break!')\n",
    "        # You **MUST** specify the dtype='str' here  \n",
    "        check_bot_dataframe = pd.DataFrame({'account':processed_account_list, 'bot_score': bot_result_list}, dtype='str')\n",
    "        check_bot_dataframe.to_csv(os.path.join(botometer_check_path, 'tweet_combined_check_first_{}.csv'.format(index+1)), \n",
    "                                   encoding='utf-8')\n",
    "        time.sleep(30) # sleep for 30 seconds\n",
    "        print('I am OK now!')\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "final_bot_likelihood_second_dataframe = pd.DataFrame({'account':processed_account_list, 'bot_score': bot_result_list}, dtype='str')\n",
    "final_bot_likelihood_second_dataframe.to_csv(os.path.join(botometer_check_path, 'tweet_combined_botometer_check.csv'), encoding='utf-8')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out the bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_likelihood_file = pd.read_csv(os.path.join(botometer_check_path, 'tweet_combined_botometer_check.csv'), encoding='utf-8', \n",
    "                                 index_col=0, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_likelihood_without_duplicates = bot_likelihood_file.drop_duplicates(subset='account', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31679, 2)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot_likelihood_without_duplicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we see the not authorized accounts as the common users, not the bots\n",
    "decision1 = (bot_likelihood_without_duplicates['bot_score'] != 'Not Authorized')\n",
    "bot_likelihood_without_not_authorized = bot_likelihood_without_duplicates.loc[decision1]\n",
    "bot_likelihood_without_not_authorized_copy = bot_likelihood_without_not_authorized.copy()\n",
    "bot_likelihood_without_not_authorized_copy['bot_score'] = bot_likelihood_without_not_authorized_copy['bot_score'].astype(float)\n",
    "# Then the loglikelihood score of bots should be greater than 0.4\n",
    "decision2 = (bot_likelihood_without_not_authorized_copy['bot_score'] > 0.4)\n",
    "bot_accounts_dataframe = bot_likelihood_without_not_authorized_copy[decision2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_account_list = list(bot_accounts_dataframe['account'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_account_int_list = [np.int64(account) for account in bot_account_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_without_bot_step1_copy = total_dataframe_without_bot_step1.copy()\n",
    "total_dataframe_without_bot_step1_copy['user_id_str'] = total_dataframe_without_bot_step1_copy['user_id_str'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_without_bot_step2 = total_dataframe_without_bot_step1_copy.loc[~total_dataframe_without_bot_step1_copy['user_id_str'].isin(bot_account_int_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweet is: 431074; Total number of user is 30836\n"
     ]
    }
   ],
   "source": [
    "number_of_tweet_user(total_dataframe_without_bot_step2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataframe_without_bot_step2.to_csv(os.path.join(tweet_combined_path, 'tweet_combined_in_hk_withoutbot_step2.csv'), \n",
    "                                         encoding='utf-8', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'en': 324167, 'zh': 106907})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(total_dataframe_without_bot_step2['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
